{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_info(review_id, review_link, http):\n",
    "    \"\"\"\n",
    "    some text\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([])\n",
    "    \n",
    "    #pattern = r\"(?<=albums/)([0-9]*)\" #thought that was review id but it seems like they got rid of it... unfortunate\n",
    "    #m = re.search(pattern, review_link)[0]\n",
    "\n",
    "    response = http.get(review_link)\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    \n",
    "    df.loc[review_id, \"album\"] = soup.find(\"h1\", class_=\"single-album-tombstone__review-title\").text\n",
    "    df.loc[review_id, \"artist\"] = \", \".join([a.text for a in soup.find(\"ul\", class_=\"single-album-tombstone__artist-links\")])\n",
    "    df.loc[review_id, \"score\"] = float(soup.find(\"span\", class_=\"score\").contents[0])\n",
    "    df.loc[review_id, \"author\"] = soup.find(\"a\", class_=\"authors-detail__display-name\").text\n",
    "    if soup.find(\"span\", class_=\"authors-detail__title\") != None:\n",
    "        df.loc[review_id, \"author_type\"] = soup.find(\"span\", class_=\"authors-detail__title\").text\n",
    "    df.loc[review_id, \"genre\"] = \", \".join([a.text for a in soup.find_all(\"a\", class_=\"genre-list__link\")])\n",
    "    df.loc[review_id, \"review_date\"] = datetime.strptime(soup.find(\"time\", class_=\"pub-date\")[\"datetime\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    if soup.find(\"li\", class_=\"labels-list__item\") != None:\n",
    "        df.loc[review_id, \"label\"] = soup.find(\"li\", class_=\"labels-list__item\").contents\n",
    "    df.loc[review_id, \"release_year\"] = soup.find(\"span\", \"single-album-tombstone__meta-year\").contents[-1]\n",
    "    if soup.find(\"p\", \"bnm-txt\") != None:\n",
    "        df.loc[review_id, \"special_label\"] = soup.find(\"p\", \"bnm-txt\").text\n",
    "    df.loc[review_id, \"content\"] = \"\\n\".join([a.text for a in soup.find(\"div\", class_=\"review-detail__article-content\").find_all(\"p\")])\n",
    "    df.loc[review_id, \"url\"] = review_link\n",
    "    return df\n",
    "    \n",
    "def find_albums(url, http):\n",
    "    \"\"\"\n",
    "    some text\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    response = http.get(url)\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    spoon = soup.find_all(\"a\", class_=\"review__link\")\n",
    "    for element in spoon:\n",
    "        review_link = \"https://pitchfork.com\" + element[\"href\"]  \n",
    "        reviews.append(review_link)\n",
    "    return reviews\n",
    "\n",
    "def scrape_pitchfork(verbose=True, number_of_pages = 5):\n",
    "    \"\"\"\n",
    "    some text\n",
    "    \"\"\"\n",
    "    \n",
    "    time_before = time.time()\n",
    "    \n",
    "    \n",
    "    #### setup    \n",
    "    base_url = \"https://pitchfork.com/reviews/albums/?page=\"\n",
    "        \n",
    "    #### retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "    \n",
    "    #### actual implementation\n",
    "    df = pd.DataFrame([])\n",
    "    reviews = []\n",
    "    review_id=1\n",
    "    for i in range(1, number_of_pages+1):\n",
    "        url = base_url + str(i)\n",
    "        review_links = find_albums(url, http)\n",
    "        for link in review_links:\n",
    "            df = df.append(get_review_info(review_id, link, http))\n",
    "            review_id += 1\n",
    "    \n",
    "            if verbose:\n",
    "                print (f\"working on page number {i} of {number_of_pages} dealing with review number {review_id}\", end=\"\\r\")\n",
    "\n",
    "        if i%50 == 0:\n",
    "            ### Save at every 50th step because sometimes the algorithm breaks\n",
    "            df.to_csv(f'1 to {i}.csv')\n",
    "            print(f\"last album was {review_links[-1]}\")\n",
    "            \n",
    "    #### timer for final print\n",
    "    elapsed_time = time.time() - time_before\n",
    "    print(f\"this approach took {elapsed_time} seconds for a total of {review_id} reviews\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The last page on pitchfork as og August 19th 2020 is 1900.\n",
    "\n",
    "That's about 22800 reviews and can take up to 6/7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last album was https://pitchfork.com/reviews/albums/449-automatic-writing/\n",
      "last album was https://pitchfork.com/reviews/albums/8519-lovers-lead-the-way/\n",
      "last album was https://pitchfork.com/reviews/albums/967-more-nipples/\n",
      "last album was https://pitchfork.com/reviews/albums/4631-light-magic/\n",
      "last album was https://pitchfork.com/reviews/albums/4447-versus/1599\n",
      "last album was https://pitchfork.com/reviews/albums/3702-neil-michael-hagerty/\n",
      "last album was https://pitchfork.com/reviews/albums/3723-left-for-dead-in-malaysia/\n",
      "this approach took 4428.469939947128 seconds for a total of 22788 reviews\n"
     ]
    }
   ],
   "source": [
    "df = scrape_pitchfork(number_of_pages = 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(f'complete list.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
